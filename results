valid
0.2591760299625468
test
0.27395

yperxe +-2-5% apoklish valid me test

tests me aplo LogisticRegression me vash 2 sthles
megisto apotelesma 0.34880 opws dwthke apo ergasia
-----------------
0 xoris encoder
tests me aplo LogisticRegression me vash 3 sthles
0, 1, 5
(C=1, max_iter=1000, tol=1e-7, solver='lbfgs', multi_class='ovr')
0.39970 0.39205

---------------------
(C=2, max_iter=2500, tol=1e-6, solver='newton-cg', multi_class='ovr')
0.40568 0.39782

C=5, max_iter=10000, tol=1e-7
0.40868 0.39141
-----------------
C=100, max_iter=5000, tol=1e-6, solver='newton-cg', multi_class='ovr', fit_intercept=True
0.41167 0.39077
-------------
C=5, max_iter=2500, tol=1e-7, solver='newton-cg', multi_class='multinomial', fit_intercept=False
0.43263 0.39654
(valid .3891385767790262)
--------------
C=10, max_iter=10000, tol=1e-7, solver='newton-cg', multi_class='multinomial', fit_intercept=False
0.43562 0.39782
valid(0.39026217228464416)
------------------
classWeight = (df_train.groupby('PAX').size()/df_train['PAX'].size).to_dict()
clf = LogisticRegression(C=500.0, max_iter=1000000, tol=1e-7, solver='newton-cg', multi_class='auto', fit_intercept=False, class_weight=classWeight)
didnotchange much
0.41317 0.39974

datesEncoding anevase epidosh shmantikothta sthn mera
1o tests
0.42292134831460676
0.44461 0.41575
C=5.0, max_iter=1000000, tol=1e-7, solver='newton-cg', multi_class='auto', fit_intercept=False


2o test
paralhpsy day
nothing

3o test
paralhpsy day+year

4o tests
mono d06
clf = LogisticRegression(C=20.0, max_iter=20000, tol=1e-6, solver='liblinear', multi_class='auto', fit_intercept=False)
0.44610 0.41127

5o tests
mono d06 months
C=1.0, max_iter=1000000, tol=1e-7, solver='newton-cg', multi_class='auto', fit_intercept=True
0.430561797752809
0.42365
c=5.0
0.44311 0.40871






'''
default ridClassifier
me yparxontes tropopoihseis default date
0.36976 0.37475
dropped not significant changes


linear classifiers maxout
https://stackoverflow.com/questions/38640109/logistic-regression-python-solvers-defintions


moving to SVM


''''
default linearsvc
me yparxontes tropopoihseis default date
0.39970 0.39718
not suprissing


'''
GAMO SUPPORT VECTOR MACHINE
me yparxontes tropopoihseis default date
clf = svm.SVC(tol=1e-5, gamma='scale', C=2.0)
0.4498876404494382
0.47754 0.45099


clf = svm.SVC(tol=1e-6, gamma='scale', C=5.0, kernel='rbf')
0.48629213483146067
0.51347 0.50224

svm.SVC(tol=1e-6, gamma='scale', C=10, kernel='rbf')
0.5119101123595505
0.53892 0.52402

c=17
0.5231460674157303
0.55389 0.52978

16.9
0.5235955056179775
0.55239 0.53042

c=21
0.5267415730337078
0.55089 0.53235
============================
only d06
0.5038202247191012
0.52095 0.50864

days/d06
0.5087640449438202
0.51347 0.50608

months/d06
0.5231460674157303
0.53742 0.51633
====================================
month/day/d06
0.5155056179775281
0.55688 0.50160

droplist = [2, 4, 6, 8, 9, 10, 11]
0.5276404494382022
0.56287 0.52978

all dates
0.5303370786516854
0.56137 0.53747

========================
clf = svm.SVC(tol=1e-6, gamma='scale', C=10000, kernel='rbf')
0.49752808988764047
0.53293 0.51761
=====================================================
month/year/d06
0.5119101123595505
0.52694 0.52017



did not work
df_train, df_test = mp.combineDepArr(df_train, df_test)

======================================================================================
1)[month/day/d06/distance]
antikatastash latlong me distance latlongdep latlongarr bump ap
0.5550561797752809
0.57035 0.56117

2)[alldays/distance][Year ephreazei arnhtika]
0.5613483146067416
0.56586 0.56822


3)[months/d06]
0.5253932584269663
0.53592 0.51249




================
RandomForestClassifier Explor
all default
flactuation in estimation if same estimator
0.44134831460674157
0.44760 0.44907


estimators = 100
0.4903370786516854
0.49550

estimators = 181
0.50149 0.51441

 RandomForestClassifier(n_estimators=100, max_depth=46, max_features=27, bootstrap=True)
 0.5006741573033708
0.50449 0.52017

month/day/d06
droplist = [2, 4, 6, 8, 9, 10, 11]
0.5155056179775281
0.51796 0.50992






=================================================================================
impact after implementing mean of reservation as dates

[month/day/d06]
SVM: 0.5379775280898876 0.52544  0.53363(tol=1e-6, C=17, kernel='rbf', gamma='scale')
LogRes:0.501123595505618 0.52544 0.49391 (C=17, max_iter=100000, tol=1e-6, solver='newton-cg', multi_class='multinomial', fit_intercept=False, random_state=42)
RanFos:  0.461123595505618 0.42814 0.45227(n_estimators=80, max_depth=20, max_features=50, bootstrap=True, random_state=42)




===================================
neural
0.54940 0.54067
model = Sequential()
model.add(Dense(8, input_dim=114, activation='relu'))
model.add(Dense(8, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])
estimator = KerasClassifier(build_fn=baseline_model, epochs=200, batch_size=5, verbose=0)



=======================================================================
=======================================================================

SVM VS NEURAL
model.add(Dense(157, activation='relu', input_dim=157))
model.add(Dropout(0.6))
model.add(Dense(16, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(8, activation='softmax'))
FYear FMonth FDay FDay0  Departure Arrival   Distance
157
SVM
kernel='rbf', gamma='scale', C=17
0.60479 0.57783

neural
sgd = SGD(lr=.01, decay=1e-6, momentum=0.9, nesterov=False)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
model.fit(X_train, y_train, epochs=200, batch_size=128)
0.61077 0.59000

ta idia woris FYear
154
SVM
0.57185 0.55541
NEURAL
0.57035 0.56438


periptwsh 1
adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
model.fit(X_train, y_train, epochs=100, batch_size=128)
0.61077 0.59320




=======
model.add(Dense(157, activation='relu', input_dim=157))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(8, activation='softmax'))
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
model.fit(X_train, y_train, epochs=130, batch_size=256)
0.62125 0.60409


sgd = SGD(lr=.01, decay=1e-6, momentum=0.9, nesterov=False)
model.fit(X_train, y_train, epochs=175, batch_size=256)
0.61676 0.58360


====================================================

model.add(Dense(157, activation='relu', input_dim=157))
model.add(Dropout(0.5))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.6))
model.add(Dense(8, activation='softmax'))

sgd = SGD(lr=.01, decay=1e-6, momentum=0.9, nesterov=False)
model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])
model.fit(X_train, y_train, epochs=300, batch_size=256)



=====================================
svm 0.57784 0.56758
neyral 0.62275 0.60345
============================================

ensomatosh ne/nw, sw,se drop 9 10 11
model.add(Dense(203, activation='relu', input_dim=203))
model.add(Dropout(0.6))
model.add(Dense(32, activation='relu'))
model.add(Dropout(0.5))
model.add(Dense(8, activation='softmax'))
adam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)
model.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])
model.fit(X_train, y_train, epochs=130, batch_size=256)
0.64670 0.59641

=============================================================
simple cardinal
FYear FMonth FDay  FD06  Departure  CityDeparture  Arrival  CityArrival  WeekOfYear  Trimester  Distance  Start  Finish

model.add(Dense(224, activation='relu', input_dim=258))
model.add(Dropout(0.7))
model.add(Dense(128, activation='relu'))
model.add(Dropout(0.6))
model.add(Dense(8, activation='softmax'))
model.summary()
model.compile(loss='categorical_crossentropy', optimizer=rmsp, metrics=['accuracy'])
model.fit(X_train, y_train, epochs=100, batch_size=256, class_weight=counts, verbose=2)
0.66467
